# RL-Project-LUH-
In a simulated insurance-underwriting task with heavy-tailed rewards and delayed returns, does ez-greedy exploration yield higher discounted return and safer convergence than fixed ϵ-greedy?

# Starting Experiment
example call: python run_experiment.py --episodes 100 --delay 5 --eps 0.2 --seed 42

# Johann, der Agentenbändiger

Inmitten von Code, so still und klar,
sitzt Johann, der stets Forscher war.
Mit States und Actions, Reward im Blick,
trainiert er den Agenten Stück für Stück.

Er schaut, wie Policy langsam lernt,
wie Q-Werte wachsen, wie alles sich entfernt
vom Zufall, vom reinen, blinden Tun –
bis Strategien wie Sterne am Himmel ruh’n.

Manch Episode schlägt fehl, doch er lacht,
denn Fehler sind’s, die Fortschritt machen sacht.
Geduldig passt er Hyperparameter an,
bis der Agent sein Ziel erreichen kann.

So sitzt er bis spät, bei Kerzenschein,
in einer Welt aus Zuständen, groß und klein,
und denkt sich: „Wie schön, wenn Maschinen verstehen –
doch am schönsten ist’s, den Weg dorthin zu gehen.“